# ============================================================
# MONGODB CONFIGURATION
# ============================================================
# Full MongoDB connection string with database name for crawler
mongo_uri=mongodb://localhost:27017/books

# MongoDB connection string without database for scheduler
mongodb_url=mongodb://localhost:27017


# ============================================================
# CRAWLER CONFIGURATION
# ============================================================
max_concurrency=10
request_timeout=10
retry_attempts=5
request_interval=1.0
user_agent=BooksCrawler/1.0 (+https://github.com/yourusername/)
store_html_in_gridfs=true


# ============================================================
# SCHEDULER CONFIGURATION
# ============================================================
reports_dir=./reports
crawl_interval=3600
logging_level=INFO


# ============================================================
# API CONFIGURATION
# ============================================================
# Server settings
API_HOST=0.0.0.0
API_PORT=8000
API_TITLE=Books to Scrape API
API_VERSION=1.0.0
API_DESCRIPTION=RESTful API for querying crawled book data

# Security - API Keys (format: hash1:description1,hash2:description2)
# Generate keys using: python scripts/generate_api_key.py
API_KEYS=your-hashed-api-key-here:description

# Rate Limiting
RATE_LIMIT_PER_HOUR=100

# CORS - Allowed origins (comma-separated or * for all)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080

# Pagination
DEFAULT_PAGE_SIZE=20
MAX_PAGE_SIZE=100

# Logging
API_LOG_LEVEL=INFO
LOG_TO_FILE=true
LOG_JSON_FORMAT=false
LOG_ROTATION=true
